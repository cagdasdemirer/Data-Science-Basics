---
title: "CustomerSegmentation"
output: html_document
---

```{r}
library(dplyr)
library(plotrix)
library(purrr)
library(cluster) 
library(gridExtra)
library(grid)
library(NbClust)
library(factoextra)

```


```{r}
data <- read.csv("Customers.csv")
head(data, 5)
dim(data)
sum(is.na(data))
summary(data)
SD <- data %>%
        select(-c(CustomerID,Gender)) %>%
          apply(2,sd)
as.data.frame(SD)

```


```{r}

a=table(data$Gender)
pct=round(a/sum(a)*100)
lbs=paste(c("Female","Male")," ",pct,"%",sep=" ")
pie3D(a,labels=lbs,
   main="Pie Chart Depicting Ratio of Female and Male")

```

```{r}

hist(data$Age,
    col="#660033",
    main="Histogram to Show Count of Age Class",
    xlab="Age Class",
    ylab="Frequency",
    labels=TRUE)

```

```{r}
plot(density(data$Annual.Income..k..),
    col="black",
    main="Density Plot for Annual Income",
    xlab="Annual Income Class",
    ylab="Density")
polygon(density(data$Annual.Income..k..),
        col="grey")
```

```{r}
boxplot(data$Spending.Score..1.100.,
   horizontal=TRUE,
   col="#2AE9EE",
   main="BoxPlot for Descriptive Analysis of Spending Score")
```

Before we use the K-means Algorithm we should determine the optimal cluster number.To help us in determining the optimal clusters, there are three popular methods:

- Elbow method
- Silhouette method
- Gap statistic 

```{r}
# Elbow method
set.seed(123)
# function to calculate total intra-cluster sum of square 
iss <- function(k) {
  kmeans(data[,3:5],k,iter.max=100,nstart=100,algorithm="Lloyd" )$tot.withinss
}
k.values <- 1:10
iss_values <- map_dbl(k.values, iss)
plot(k.values, iss_values,
    type="b", pch = 19, frame = FALSE, 
    xlab="Number of clusters K",
    ylab="Total intra-clusters sum of squares")

```

```{r}
# Average Silhouette method
k2<-kmeans(data[,3:5],2,iter.max=100,nstart=50,algorithm="Lloyd")
s2<-plot(silhouette(k2$cluster,dist(data[,3:5],"euclidean")))

k3<-kmeans(data[,3:5],3,iter.max=100,nstart=50,algorithm="Lloyd")
s3<-plot(silhouette(k3$cluster,dist(data[,3:5],"euclidean")))

k4<-kmeans(data[,3:5],4,iter.max=100,nstart=50,algorithm="Lloyd")
s4<-plot(silhouette(k4$cluster,dist(data[,3:5],"euclidean")))

k5<-kmeans(data[,3:5],5,iter.max=100,nstart=50,algorithm="Lloyd")
s5<-plot(silhouette(k5$cluster,dist(data[,3:5],"euclidean")))

k6<-kmeans(data[,3:5],6,iter.max=100,nstart=50,algorithm="Lloyd")
s6<-plot(silhouette(k6$cluster,dist(data[,3:5],"euclidean")))

k7<-kmeans(data[,3:5],7,iter.max=100,nstart=50,algorithm="Lloyd")
s7<-plot(silhouette(k7$cluster,dist(data[,3:5],"euclidean")))

k8<-kmeans(data[,3:5],8,iter.max=100,nstart=50,algorithm="Lloyd")
s8<-plot(silhouette(k8$cluster,dist(data[,3:5],"euclidean")))

k9<-kmeans(data[,3:5],9,iter.max=100,nstart=50,algorithm="Lloyd")
s9<-plot(silhouette(k9$cluster,dist(data[,3:5],"euclidean")))

k10<-kmeans(data[,3:5],10,iter.max=100,nstart=50,algorithm="Lloyd")
s10<-plot(silhouette(k10$cluster,dist(data[,3:5],"euclidean")))

fviz_nbclust(data[,3:5], kmeans, method = "silhouette")

```


```{r}
# GAP Statistic method

set.seed(125)
stat_gap <- clusGap(data[,3:5], FUN = kmeans, nstart = 25,
            K.max = 10, B = 50)
fviz_gap_stat(stat_gap)

```

Let us take k = 6 as our optimal cluster 
 
```{r}
k6
```

Visualizing the Clustering Results using the First Two Principle Components

```{r}
pcclust <- prcomp(data[,3:5],scale=FALSE) #principal component analysis
summary(pcclust)
pcclust$rotation[,1:2]

set.seed(132)
ggplot(data, aes(x =Annual.Income..k.., y = Spending.Score..1.100.)) + 
  geom_point(stat = "identity", aes(color = as.factor(k6$cluster))) +
  scale_color_discrete(name=" ",
              breaks=c("1", "2", "3", "4", "5","6"),
              labels=c("Cluster 1", "Cluster 2", "Cluster 3", "Cluster 4", "Cluster 5","Cluster 6")) +
  ggtitle("Segments of Customers", subtitle = "Using K-means Clustering")


ggplot(data, aes(x =Spending.Score..1.100., y =Age)) + 
  geom_point(stat = "identity", aes(color = as.factor(k6$cluster))) +
  scale_color_discrete(name=" ",
                      breaks=c("1", "2", "3", "4", "5","6"),
                      labels=c("Cluster 1", "Cluster 2", "Cluster 3", "Cluster 4", "Cluster 5","Cluster 6")) +
  ggtitle("Segments of Customers", subtitle = "Using K-means Clustering")


kCols <- function(vec){cols=rainbow(length(unique(vec)))
            return (cols[as.numeric(as.factor(vec))])}
digCluster <- k6$cluster; dignm <- as.character(digCluster); # K-means clusters

plot(pcclust$x[,1:2], col = kCols(digCluster),pch =19,xlab ="K-means",ylab="classes")
legend("bottomleft",unique(dignm),fill=unique(kCols(digCluster)))

# "classes" represents PCA2 and "K-means" represents PCA1.
```


